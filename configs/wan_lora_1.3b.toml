# ───────────  GLOBAL (root-level)  ───────────
output_dir                      = '/workspace/output/wan_lora_1.3b'
dataset                         = '/workspace/diffusion-pipe/configs/dataset.toml'
epochs                          = 15
micro_batch_size_per_gpu        = 8
image_micro_batch_size_per_gpu  = 32
gradient_accumulation_steps     = 1
save_every_n_steps             = 250
gradient_clipping               = 1.0
activation_checkpointing        = 'unsloth'
# blocks_to_swap                  = 32
steps_per_print                 = 1
video_clip_mode                 = 'single_middle'
mixed_precision                 = "bf16"
pipeline_stages                 = 1
warmup_steps = 500

#########################################
#  MODEL CONFIG for Wan2.1 T2V fine-tune
#########################################
[model]
type = "wan"
ckpt_path = "/workspace/models/Wan2.1-T2V-1.3B"
llm_path = "/workspace/models/Wan2.1-T2V-1.3B/models_t5_umt5-xxl-enc-bf16.pth"
dtype = "bfloat16"
cache_text_embeddings = false           # compute per-step
train_text_encoder = true               # NEW
save_text_encoder_full = true
text_train_blocks = 8                   # train the last 4 encoder blocks
freeze_transformer = true
save_diffusion_model = false

[model.caption_aug]
enable           = true
case_insensitive = true
single_token     = true         # new
trim_prob        = 0.90         # start very high
# ramp down as TE improves single-token comprehension
trim_prob_schedule = [
  [5.0, 0.80],
  [10.0, 0.60],
  [20.0, 0.40],
  [30.0, 0.25],
]
keywords = [
  # expand to include common variants/plurals/slang
  "breast","breasts","boob","boobs","tit","tits","nipple","nipples","areola","areolae",
  "penis","cock","dick","shaft","glans","scrotum","testicle","testicles",
  "vagina","vulva","labia","clit","clitoris","pussy",
  "anus","butt","ass","buttocks","rear","hips",
  "sex","oral","blowjob","handjob","penetration","penetrating","ride","riding","doggy"
]

#[adapter]
#type    = "lora"
#rank    = 16
#dropout = 0.1
# Optional: train LoRA only on upper blocks (we'll freeze others in code)
#train_blocks_range = [12, 29]

[optimizer]
type = "AdamW8bitKahan"
lr = 4e-5          # default for DiT/LoRA
lr_te = 6e-6       # slower for the last 4 TE blocks you’re training
weight_decay = 0.01

[timer]
# Make sure logging shows step time
log_time              = true
